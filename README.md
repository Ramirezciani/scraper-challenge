# Scraper Challenge â€“ Jurisprudencia PJ

Scraper desarrollado en **TypeScript** como parte de un desafÃ­o tÃ©cnico de scraping.
El objetivo es extraer informaciÃ³n estructurada y documentos PDF desde el portal de
jurisprudencia del Poder Judicial del PerÃº, sin utilizar automatizaciÃ³n de navegador.

---

## ğŸ¯ Objetivo del desafÃ­o

El scraper debe ser capaz de:

- Navegar por todas las pÃ¡ginas de resultados del portal
- Extraer la informaciÃ³n disponible de cada documento
- Descargar los PDFs asociados
- Manejar errores de rate limiting (HTTP 429) mediante reintentos con backoff
- Continuar la ejecuciÃ³n aunque existan fallos puntuales

El foco del desafÃ­o no es la descarga masiva, sino el **diseÃ±o de un scraper robusto,
mantenible y bien estructurado**.

---

## ğŸŒ Sitio objetivo

URL base del portal: https://jurisprudencia.pj.gob.pe/jurisprudenciaweb/faces/page/resultado.xhtml

### Consideraciones importantes

El portal presenta caracterÃ­sticas propias de sistemas legacy:

- Uso de **JavaServer Faces (JSF)** y `ViewState`
- NavegaciÃ³n basada en estado y sesiones
- Accesos condicionados por cookies, headers y contexto
- Posibles bloqueos HTTP 403 dependiendo del entorno

Estas condiciones forman parte del desafÃ­o y fueron consideradas en el diseÃ±o
de la arquitectura del scraper.

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

- Node.js
- TypeScript
- Axios
- Cheerio

No se utilizan librerÃ­as de automatizaciÃ³n de navegador
(Puppeteer, Playwright, Selenium).

---

## ğŸ“ Estructura del proyecto

```text
scraper-challenge/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts                 # Orquestador principal
â”‚   â”‚
â”‚   â”œâ”€â”€ init/
â”‚   â”‚   â””â”€â”€ session.ts           # InicializaciÃ³n de sesiÃ³n / contexto
â”‚   â”‚
â”‚   â”œâ”€â”€ client/
â”‚   â”‚   â””â”€â”€ http.ts              # Cliente HTTP (cookies, headers, retry)
â”‚   â”‚
â”‚   â”œâ”€â”€ crawler/
â”‚   â”‚   â”œâ”€â”€ paginator.ts         # NavegaciÃ³n / paginaciÃ³n JSF
â”‚   â”‚   â”œâ”€â”€ extractor.ts         # ExtracciÃ³n de datos desde HTML
â”‚   â”‚   â””â”€â”€ pdfDownloader.ts     # Descarga de PDFs (429 + backoff)
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ writer.ts            # Persistencia de datos
â”‚   â”‚   â””â”€â”€ failed.ts            # Registro de descargas fallidas
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.ts            # Logging
â”‚   â”‚   â””â”€â”€ sleep.ts             # Delays / backoff
â”‚   â”‚
â”‚   â””â”€â”€ types/
â”‚       â””â”€â”€ Document.ts          # Tipos de dominio
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ json/                    # Datos extraÃ­dos
â”‚   â””â”€â”€ pdf/                     # PDFs descargados
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```
---

## âš™ï¸ InstalaciÃ³n

    ```bash
    npm install

    â–¶ï¸ EjecuciÃ³n

    Modo desarrollo:
    npm run dev

    Build y ejecuciÃ³n:
    npm run build
    npm start
    ```

## âš™ï¸ Descarga de PDFs y manejo de errores  

COMPORTAMIENTO:
 - Las descargs de PDFs detectan errores HTTP 429
 - Se aplican reintentos con backoff exponencial
 - Los fallos persistentes se registran para reintento posterior
 - El scraper continÃºa procesando otros documentos

## Notas Finales 
Este proyecto fue diseÃ±ado para demostrar:
  - AnÃ¡lisis de sistemas legacy
  - Manejo de estado y sesiones
  - DiseÃ±o modular y mantenible
  - Tolerancia a fallos en procesos de larga duraciÃ³n
  - DocumentaciÃ³n tÃ©cnica clara