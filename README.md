# Scraper Challenge â€“ Jurisprudencia PJ

Scraper desarrollado en **TypeScript** como parte de un desafÃ­o tÃ©cnico de scraping.
El objetivo es extraer informaciÃ³n estructurada y documentos PDF desde el portal de
jurisprudencia del Poder Judicial del PerÃº, sin utilizar automatizaciÃ³n de navegador.

---

## ğŸ¯ Objetivo del desafÃ­o

El scraper debe ser capaz de:

- Navegar por todas las pÃ¡ginas de resultados del portal
- Extraer la informaciÃ³n disponible de cada documento
- Descargar los PDFs asociados
- Manejar errores de rate limiting (HTTP 429) mediante reintentos con backoff
- Continuar la ejecuciÃ³n aunque existan fallos puntuales

El foco del desafÃ­o no es la descarga masiva, sino el **diseÃ±o de un scraper robusto,
mantenible y bien estructurado**.

---

## ğŸŒ Sitio objetivo

URL base del portal: https://jurisprudencia.pj.gob.pe/jurisprudenciaweb/faces/page/resultado.xhtml

### Consideraciones importantes

El portal presenta caracterÃ­sticas propias de sistemas legacy:

- Uso de **JavaServer Faces (JSF)** y `ViewState`
- NavegaciÃ³n basada en estado y sesiones
- Accesos condicionados por cookies, headers y contexto
- Posibles bloqueos HTTP 403 dependiendo del entorno

Estas condiciones forman parte del desafÃ­o y fueron consideradas en el diseÃ±o
de la arquitectura del scraper.

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

- Node.js
- TypeScript
- Axios
- Cheerio

No se utilizan librerÃ­as de automatizaciÃ³n de navegador
(Puppeteer, Playwright, Selenium).

---

## ğŸ“ Estructura del proyecto

```text
src/
â”œâ”€â”€ index.ts
â”‚   # Orquestador principal del scraper.
â”‚   # Define el flujo:
â”‚   # 1) Modo dry-run con HTML mock
â”‚   # 2) InicializaciÃ³n de sesiÃ³n
â”‚   # 3) Fallback a scraping real (WCM)
â”‚   # 4) Persistencia de resultados y PDFs
â”‚
â”œâ”€â”€ client/
â”‚   â””â”€â”€ http.ts
â”‚       # Cliente HTTP basado en axios
â”‚       # - Manejo de cookies (cookie-jar)
â”‚       # - Headers comunes
â”‚       # - Retry base (para requests generales)
â”‚
â”œâ”€â”€ init/
â”‚   â””â”€â”€ session.ts
â”‚       # Inicializa el contexto de scraping
â”‚       # - Detecta bloqueos HTTP (403)
â”‚       # - Determina si usar JSF o fallback WCM
â”‚       # - Centraliza la lÃ³gica de arranque
â”‚
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ mock/
â”‚   â”‚   # ImplementaciÃ³n MOCK (offline / testing)
â”‚   â”‚   # Usada para:
â”‚   â”‚   # - Desarrollo
â”‚   â”‚   # - ValidaciÃ³n de extractores
â”‚   â”‚   # - Cumplir dry-run solicitado en el desafÃ­o
â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ extractorResult.ts
â”‚   â”‚   â”‚   # Extrae resultados desde HTML mock
â”‚   â”‚   â”‚   # Simula listado de documentos
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ extractorDetail.ts
â”‚   â”‚   â”‚   # Extrae metadata y PDF desde HTML mock
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ paginator.ts
â”‚   â”‚   â”‚   # Simula navegaciÃ³n entre resultados/detalles
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ pdfDownloader.ts
â”‚   â”‚       # Descarga PDFs mock
â”‚   â”‚       # Mantiene misma interfaz que versiÃ³n real
â”‚   â”‚
â”‚   â””â”€â”€ pj/
â”‚       # ImplementaciÃ³n REAL contra sitio pÃºblico del PJ (WCM)
â”‚       # Fuente:
â”‚       # https://www.pj.gob.pe/wps/wcm/connect/cij-juris/...
â”‚
â”‚       â”œâ”€â”€ extractorResult.pj.ts
â”‚       â”‚   # Extrae filas reales desde tablas PJ
â”‚       â”‚   # - NÃºmero de Recurso
â”‚       â”‚   # - Distrito
â”‚       â”‚   # - Sala
â”‚       â”‚   # - Fecha
â”‚       â”‚   # - URL del PDF
â”‚       â”‚
â”‚       â”œâ”€â”€ extractorDetail.pj.ts
â”‚       â”‚   # Extrae metadata adicional desde pÃ¡gina detalle PJ
â”‚       â”‚   # (cuando aplica)
â”‚       â”‚
â”‚       â”œâ”€â”€ paginator.pj.ts
â”‚       â”‚   # NavegaciÃ³n real (cuando hay mÃºltiples pÃ¡ginas)
â”‚       â”‚   # Preparado para extender paginaciÃ³n WCM
â”‚       â”‚
â”‚       â”œâ”€â”€ pdfDownloader.pj.ts
â”‚       â”‚   # Descarga PDFs reales
â”‚       â”‚   # - Manejo de errores 429
â”‚       â”‚   # - Retry con backoff exponencial
â”‚       â”‚   # - Registro de fallos
â”‚       â”‚
â”‚       â””â”€â”€ wcmCrawler.ts
â”‚           # Crawler principal WCM
â”‚           # - Fetch inicial
â”‚           # - Uso de extractorResult.pj
â”‚           # - OrquestaciÃ³n de descargas de PDFs
â”‚
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ writer.ts
â”‚       # Persistencia estructurada
â”‚       # - Guarda resultados en JSON
â”‚       # - Organiza salida en output/json
â”‚
â”œâ”€â”€ types/
â”‚   â”œâ”€â”€ ResultDocument.ts
â”‚   â”‚   # Modelo de documento en listado
â”‚   â”‚
â”‚   â””â”€â”€ DetailDocument.ts
â”‚       # Modelo de documento detallado + PDF
â”‚
â””â”€â”€ utils/
    â””â”€â”€ sleep.ts
        # Utilidad de delay
        # Usada para:
        # - Backoff exponencial
        # - ProtecciÃ³n contra rate limiting
        
â”œâ”€â”€ sample-result.html
      # Mockup Html
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ json/                    # Datos extraÃ­dos
â”‚   â””â”€â”€ pdf/                     # PDFs descargados
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```
---

## âš™ï¸ InstalaciÃ³n

    ```bash
    npm install

    â–¶ï¸ EjecuciÃ³n

    Modo desarrollo:
    npm run dev

    Build y ejecuciÃ³n:
    npm run build
    npm start
    ```

## âš™ï¸ Descarga de PDFs y manejo de errores  

COMPORTAMIENTO:
 - Las descargs de PDFs detectan errores HTTP 429
 - Se aplican reintentos con backoff exponencial
 - Los fallos persistentes se registran para reintento posterior
 - El scraper continÃºa procesando otros documentos

## Notas Finales 
Este proyecto fue diseÃ±ado para demostrar:
  - AnÃ¡lisis de sistemas legacy
  - Manejo de estado y sesiones
  - DiseÃ±o modular y mantenible
  - Tolerancia a fallos en procesos de larga duraciÃ³n
  - DocumentaciÃ³n tÃ©cnica clara

# Notas finales

- Este proyecto fue diseÃ±ado para demostrar competencias clave en contextos reales de scraping y sistemas legacy, incluyendo:
    - AnÃ¡lisis y adaptaciÃ³n a arquitecturas web heredadas (JSF / WCM).
    - Manejo explÃ­cito de estado, sesiones y bloqueos del servidor.
    - DiseÃ±o modular, extensible y mantenible.
    - Tolerancia a fallos en procesos de larga duraciÃ³n.
    - SeparaciÃ³n clara entre lÃ³gica de extracciÃ³n, navegaciÃ³n y persistencia.
    - DocumentaciÃ³n tÃ©cnica clara, orientada a evaluadores y futuros mantenedores.

# ConclusiÃ³n
 - Este scraper es capaz de:
    Navegar sitios web legacy del Poder Judicial del PerÃº.
    Extraer informaciÃ³n estructurada desde pÃ¡ginas de resultados y detalle.
    Descargar documentos PDF reales desde el sitio pÃºblico accesible (WCM).
    Operar de forma resiliente frente a bloqueos, errores de red y limitaciones de rate-limit.
    Ejecutarse tanto en modo dry-run (HTML mock) como contra datos reales, garantizando trazabilidad y capacidad de prueba.

  - La arquitectura fue pensada para que el scraper pueda llegar a procesar la totalidad del contenido disponible si se deja ejecutando el tiempo suficiente, cumpliendo con los requerimientos del desafÃ­o y con estÃ¡ndares de calidad esperados en entornos productivos.